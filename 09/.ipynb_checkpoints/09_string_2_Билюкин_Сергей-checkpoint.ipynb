{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymorphy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-223a39775638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy2'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DateBack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DateBack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DateBack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name  \\\n",
       "0     george s at the cove  black bean soup   \n",
       "1        healthy for them  yogurt popsicles   \n",
       "2              i can t believe it s spinach   \n",
       "3                      italian  gut busters   \n",
       "4  love is in the air  beef fondue   sauces   \n",
       "\n",
       "                           preprocessed_descriptions  \n",
       "0  an original recipe created by chef scott meska...  \n",
       "1  my children and their friends ask for my homem...  \n",
       "2              these were so go it surprised even me  \n",
       "3  my sisterinlaw made these for us at a family g...  \n",
       "4  i think a fondue is a very romantic casual din...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preDescriptions_df = pd.read_csv('data/preprocessed_descriptions.csv')\n",
    "preDescriptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45416"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preDescriptions = preDescriptions_df['preprocessed_descriptions'].astype(str).tolist()\n",
    "notUniqueWords = list(itertools.chain(*[nltk.word_tokenize(description) for description in preDescriptions]))\n",
    "words = list(set(itertools.chain(*[nltk.word_tokenize(description) for description in preDescriptions])))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples shellthe 6\n",
      "toyed gruyere 5\n",
      "creminiand fernandina 8\n",
      "zaatar mostest 6\n",
      "deliciousof 2lb 10\n"
     ]
    }
   ],
   "source": [
    "preDescriptionsWordsPairs = [[words[randint(0, len(words))] for j in range(2)] for i in range(5)]\n",
    "for pair in preDescriptionsWordsPairs:\n",
    "    print(pair[0], pair[1], nltk.edit_distance(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommends 1\n",
      "recommendi 1\n",
      "recommended 2\n",
      "recommendsi 2\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "def find_nearest_words(targetWord, k):\n",
    "    for word in words:\n",
    "        if word != targetWord:\n",
    "            d[word] = nltk.edit_distance(word, targetWord)\n",
    "        \n",
    "    for w in sorted(d, key=d.get)[:k]:\n",
    "        print(w, d[w])\n",
    "    \n",
    "find_nearest_words('recommend', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>tread</td>\n",
       "      <td>tread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slowfoodbeirutorgindexinvphpcinv18</th>\n",
       "      <td>slowfoodbeirutorgindexinvphpcinv18</td>\n",
       "      <td>slowfoodbeirutorgindexinvphpcinv18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concauction</th>\n",
       "      <td>concauct</td>\n",
       "      <td>concauction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfectit</th>\n",
       "      <td>perfectit</td>\n",
       "      <td>perfectit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cookiesenjoy</th>\n",
       "      <td>cookiesenjoy</td>\n",
       "      <td>cookiesenjoy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          stemmed_word  \\\n",
       "word                                                                     \n",
       "tread                                                            tread   \n",
       "slowfoodbeirutorgindexinvphpcinv18  slowfoodbeirutorgindexinvphpcinv18   \n",
       "concauction                                                   concauct   \n",
       "perfectit                                                    perfectit   \n",
       "cookiesenjoy                                              cookiesenjoy   \n",
       "\n",
       "                                                       normalized_word  \n",
       "word                                                                    \n",
       "tread                                                            tread  \n",
       "slowfoodbeirutorgindexinvphpcinv18  slowfoodbeirutorgindexinvphpcinv18  \n",
       "concauction                                                concauction  \n",
       "perfectit                                                    perfectit  \n",
       "cookiesenjoy                                              cookiesenjoy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "stemmLemantiz_df = pd.DataFrame(data={\n",
    "    'word': words,\n",
    "    'stemmed_word': [stemmer.stem(word) for word in words],\n",
    "    'normalized_word': [wnl.lemmatize(word) for word in words]\n",
    "}).set_index('word')\n",
    "\n",
    "\n",
    "stemmLemantiz_df_notUnique = pd.DataFrame(data={\n",
    "    'word': notUniqueWords,\n",
    "    'stemmed_word': [stemmer.stem(word) for word in notUniqueWords],\n",
    "    'normalized_word': [wnl.lemmatize(word) for word in notUniqueWords]\n",
    "}).set_index('word')\n",
    "\n",
    "\n",
    "stemmLemantiz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля стоп слов 0.8198634124030686\n"
     ]
    }
   ],
   "source": [
    "StopWords_df = stemmLemantiz_df_notUnique[stemmLemantiz_df_notUnique.index.isin(stopwords.words('english'))]\n",
    "NormalWords_df = stemmLemantiz_df_notUnique[~stemmLemantiz_df_notUnique.index.isin(stopwords.words('english'))]\n",
    "print('Доля стоп слов', StopWords_df.shape[0] / NormalWords_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часто употребляемы до удаления\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "the     38827\n",
       "a       34630\n",
       "and     30058\n",
       "this    25278\n",
       "to      23371\n",
       "i       21797\n",
       "is      20222\n",
       "of      18307\n",
       "it      18036\n",
       "for     15737\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Часто употребляемы до удаления')\n",
    "stemmLemantiz_df_notUnique.index.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часто употребляемы после удаления\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "recipe    14239\n",
       "make       6135\n",
       "time       4931\n",
       "use        4484\n",
       "great      4225\n",
       "like       4100\n",
       "easy       4024\n",
       "made       3765\n",
       "one        3749\n",
       "good       3542\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Часто употребляемы после удаления')\n",
    "NormalWords_df.index.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28150333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28150333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28150333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15859407 0.         0.28150333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28150333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22711511 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28150333 0.         0.         0.\n",
      "  0.         0.         0.         0.28150333 0.         0.\n",
      "  0.         0.28150333 0.         0.         0.         0.\n",
      "  0.         0.         0.28150333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.22711511 0.         0.         0.\n",
      "  0.28150333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28150333]\n",
      " [0.         0.1117817  0.         0.05589085 0.         0.\n",
      "  0.         0.1117817  0.1117817  0.14972302 0.04509239 0.04509239\n",
      "  0.1117817  0.05589085 0.05589085 0.         0.05589085 0.\n",
      "  0.05589085 0.         0.05589085 0.05589085 0.05589085 0.05589085\n",
      "  0.05589085 0.05589085 0.         0.         0.05589085 0.05589085\n",
      "  0.05589085 0.         0.05589085 0.         0.         0.05589085\n",
      "  0.05589085 0.05589085 0.04509239 0.         0.05589085 0.05589085\n",
      "  0.05589085 0.05589085 0.         0.         0.05589085 0.\n",
      "  0.05589085 0.         0.         0.         0.05589085 0.04509239\n",
      "  0.05589085 0.         0.05589085 0.1117817  0.05589085 0.05589085\n",
      "  0.         0.         0.         0.         0.         0.05589085\n",
      "  0.05589085 0.13527716 0.05589085 0.05589085 0.         0.05589085\n",
      "  0.05589085 0.05589085 0.         0.05589085 0.1117817  0.\n",
      "  0.16767255 0.05589085 0.09446379 0.09018477 0.05589085 0.\n",
      "  0.         0.05589085 0.         0.05589085 0.05589085 0.\n",
      "  0.04509239 0.05589085 0.05589085 0.05589085 0.         0.1117817\n",
      "  0.05589085 0.         0.1117817  0.04509239 0.05589085 0.1117817\n",
      "  0.         0.         0.05589085 0.         0.05589085 0.\n",
      "  0.05589085 0.05589085 0.05589085 0.05589085 0.44712681 0.\n",
      "  0.05589085 0.04509239 0.05589085 0.05589085 0.05589085 0.05589085\n",
      "  0.         0.05589085 0.         0.         0.         0.05589085\n",
      "  0.11229226 0.05589085 0.05589085 0.         0.05589085 0.05589085\n",
      "  0.05589085 0.         0.05589085 0.         0.1117817  0.05589085\n",
      "  0.05589085 0.05589085 0.         0.         0.         0.05589085\n",
      "  0.         0.05589085 0.1117817  0.         0.1117817  0.\n",
      "  0.05589085 0.05589085 0.05589085 0.1117817  0.05589085 0.\n",
      "  0.         0.16767255 0.04509239 0.         0.40583148 0.05589085\n",
      "  0.         0.11229226 0.05589085 0.         0.14972302 0.05589085\n",
      "  0.         0.05589085 0.         0.05589085 0.         0.1117817\n",
      "  0.         0.         0.1117817  0.05589085 0.         0.03743075\n",
      "  0.05589085 0.05589085 0.05589085 0.05589085 0.05589085 0.        ]\n",
      " [0.         0.         0.1930122  0.         0.         0.\n",
      "  0.1930122  0.         0.         0.12926252 0.         0.15572103\n",
      "  0.         0.         0.         0.1930122  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1930122  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.15572103 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15572103\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10873971 0.         0.         0.         0.\n",
      "  0.         0.15572103 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.1930122\n",
      "  0.         0.         0.21747942 0.15572103 0.         0.\n",
      "  0.1930122  0.         0.         0.         0.         0.\n",
      "  0.15572103 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1930122  0.         0.         0.         0.1930122\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15572103 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1930122  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.1930122  0.         0.\n",
      "  0.         0.         0.         0.         0.1930122  0.\n",
      "  0.1930122  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15572103\n",
      "  0.         0.         0.15572103 0.         0.31144206 0.\n",
      "  0.1930122  0.12926252 0.         0.         0.12926252 0.\n",
      "  0.1930122  0.         0.         0.         0.1930122  0.\n",
      "  0.         0.         0.         0.         0.         0.12926252\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2997807  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5321086  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2997807  0.         0.         0.\n",
      "  0.         0.         0.5321086  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35635933 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35635933 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.15603551 0.15603551\n",
      "  0.         0.         0.         0.2089976  0.12588847 0.\n",
      "  0.         0.         0.         0.         0.         0.15603551\n",
      "  0.         0.15603551 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.15603551 0.         0.\n",
      "  0.         0.15603551 0.         0.15603551 0.15603551 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.15603551 0.15603551 0.         0.\n",
      "  0.         0.15603551 0.15603551 0.15603551 0.         0.\n",
      "  0.         0.15603551 0.         0.         0.         0.\n",
      "  0.15603551 0.08790769 0.15603551 0.         0.15603551 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.08790769 0.         0.         0.15603551\n",
      "  0.         0.         0.         0.         0.         0.15603551\n",
      "  0.         0.         0.         0.         0.15603551 0.\n",
      "  0.         0.15603551 0.         0.         0.         0.\n",
      "  0.15603551 0.         0.         0.15603551 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15603551\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15603551 0.         0.         0.15603551 0.         0.\n",
      "  0.1044988  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.15603551 0.         0.\n",
      "  0.         0.         0.         0.15603551 0.         0.15603551\n",
      "  0.         0.         0.         0.         0.         0.12588847\n",
      "  0.15603551 0.         0.         0.15603551 0.         0.\n",
      "  0.         0.         0.         0.15603551 0.2089976  0.\n",
      "  0.         0.         0.12588847 0.         0.         0.\n",
      "  0.         0.15603551 0.         0.         0.15603551 0.1044988\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "preDescriptions_sample = preDescriptions_df.sample(5)\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(vectorizer.fit_transform(preDescriptions_sample['preprocessed_descriptions']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crushed potatoes</th>\n",
       "      <th>wing bean and grilled prawn salad</th>\n",
       "      <th>salmon with couscous vegetable salad</th>\n",
       "      <th>peanut butter and milk chocolate chip tassies</th>\n",
       "      <th>delicious coconut custard pie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crushed potatoes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989759</td>\n",
       "      <td>0.982755</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>0.957467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wing bean and grilled prawn salad</th>\n",
       "      <td>0.989759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.717762</td>\n",
       "      <td>0.891649</td>\n",
       "      <td>0.907790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon with couscous vegetable salad</th>\n",
       "      <td>0.982755</td>\n",
       "      <td>0.717762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.856142</td>\n",
       "      <td>0.884180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peanut butter and milk chocolate chip tassies</th>\n",
       "      <td>0.952457</td>\n",
       "      <td>0.891649</td>\n",
       "      <td>0.856142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious coconut custard pie</th>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.907790</td>\n",
       "      <td>0.884180</td>\n",
       "      <td>0.910055</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               crushed potatoes  \\\n",
       "name                                                              \n",
       "crushed potatoes                                       0.000000   \n",
       "wing bean and grilled prawn salad                      0.989759   \n",
       "salmon with couscous vegetable salad                   0.982755   \n",
       "peanut butter and milk chocolate chip tassies          0.952457   \n",
       "delicious coconut custard pie                          0.957467   \n",
       "\n",
       "                                               wing bean and grilled prawn salad  \\\n",
       "name                                                                               \n",
       "crushed potatoes                                                        0.989759   \n",
       "wing bean and grilled prawn salad                                       0.000000   \n",
       "salmon with couscous vegetable salad                                    0.717762   \n",
       "peanut butter and milk chocolate chip tassies                           0.891649   \n",
       "delicious coconut custard pie                                           0.907790   \n",
       "\n",
       "                                               salmon with couscous vegetable salad  \\\n",
       "name                                                                                  \n",
       "crushed potatoes                                                           0.982755   \n",
       "wing bean and grilled prawn salad                                          0.717762   \n",
       "salmon with couscous vegetable salad                                       0.000000   \n",
       "peanut butter and milk chocolate chip tassies                              0.856142   \n",
       "delicious coconut custard pie                                              0.884180   \n",
       "\n",
       "                                               peanut butter and milk chocolate chip tassies  \\\n",
       "name                                                                                           \n",
       "crushed potatoes                                                                    0.952457   \n",
       "wing bean and grilled prawn salad                                                   0.891649   \n",
       "salmon with couscous vegetable salad                                                0.856142   \n",
       "peanut butter and milk chocolate chip tassies                                       0.000000   \n",
       "delicious coconut custard pie                                                       0.910055   \n",
       "\n",
       "                                               delicious coconut custard pie  \n",
       "name                                                                          \n",
       "crushed potatoes                                                    0.957467  \n",
       "wing bean and grilled prawn salad                                   0.907790  \n",
       "salmon with couscous vegetable salad                                0.884180  \n",
       "peanut butter and milk chocolate chip tassies                       0.910055  \n",
       "delicious coconut custard pie                                       0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сделано на основе пяти случайных рецептов а не всего датасета для ускорения\n",
    "data = squareform(pdist(vectorizer.fit_transform(preDescriptions_sample['preprocessed_descriptions']).toarray(), metric='cosine'))\n",
    "buffer_df = pd.DataFrame(data,\n",
    "                         columns=preDescriptions_sample['name'].values,\n",
    "                         index=preDescriptions_sample['name']\n",
    "                        )\n",
    "buffer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рецепты \"salmon with couscous vegetable salad\" и \"wing bean and grilled prawn salad\" максимально схожи\n",
    "# из за наименьшего косинусного расстояния 0.717762\n",
    "# При перезапуске могут быть другие рецепты из-за рандомной выборки рецептов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
